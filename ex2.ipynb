{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aerX1UMcTY7GwhvYGD4oE2P5VMNgmtq8","timestamp":1668976328562}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"PrJjwaNe2ICs","executionInfo":{"status":"ok","timestamp":1669297634415,"user_tz":-120,"elapsed":313,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}}},"source":["# import libraries\n","import torch\n","from torch import nn\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SF7oRwHU2a1Y","executionInfo":{"status":"ok","timestamp":1669297635851,"user_tz":-120,"elapsed":7,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"ddb73c0b-3712-4881-f272-bca6fd5bb0d0"},"source":["# set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","metadata":{"id":"AENQ5Vjx2dQf","executionInfo":{"status":"ok","timestamp":1669297636661,"user_tz":-120,"elapsed":1,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}}},"source":["# def linear class from mat layers cal\n","class Linear(torch.nn.Module):\n","  def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n","    factory_kwargs = {'device': device, 'dtype': dtype}\n","    super(Linear, self).__init__()\n","    self.in_features = in_features\n","    self.out_features = out_features\n","    self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n","    if bias:\n","        self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs))\n","    else:\n","        self.register_parameter('bias', None)\n","    self.reset_parameters()\n","\n","  def reset_parameters(self) -> None:\n","    self.weight = nn.Parameter(torch.rand([self.out_features, self.in_features]))\n","    if self.bias is not None:\n","      self.bias = nn.Parameter(torch.rand([self.out_features]))\n","\n","  def forward(self, input: torch.Tensor) -> torch.Tensor:\n","    return torch.matmul(input, torch.transpose(self.weight,0,1)) + self.bias\n","\n","  def extra_repr(self) -> str:\n","    return 'in_features={}, out_features={}, bias={}'.format(\n","        self.in_features, self.out_features, self.bias is not None\n","      )\n","  "],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUoNRN-B2pV3","executionInfo":{"status":"ok","timestamp":1669297640352,"user_tz":-120,"elapsed":412,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}}},"source":["# def BTU func to round out cal\n","class BTU(torch.nn.Module):\n","  def __init__(self, T=0.2, inplace: bool = False):\n","      super(BTU, self).__init__()\n","      self.T = T\n","\n","  def forward(self, input: torch.Tensor) -> torch.Tensor:\n","      return 1 / (1 + torch.exp(-input/self.T))"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYkof6z42wbl","executionInfo":{"status":"ok","timestamp":1669297641889,"user_tz":-120,"elapsed":410,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}}},"source":["# def constants for trials\n","dim = 2\n","out_dim = 1\n","num_epocs = 40000\n","# def trial variable\n","num_hidden_options = [2, 4]\n","l_rate_options = [0.01, 0.1]\n","bypass_options = [False, True]\n","# create train data and target\n","\n","# set train data\n","x_train = torch.tensor([[1, 0.1], [1, 0.9], [0.9, 0.9], [0.1, 0.9]], requires_grad=True, dtype=torch.float32)\n","t_train = torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)\n","x_table = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], requires_grad=True, dtype=torch.float32)\n","t_table = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n","# set validation data\n","x_validation = torch.concat((x_train, x_table),dim = 0)\n","t_validation = torch.concat((t_train, t_table), dim = 0)\n","\n","# def network0\n","class XOR_Net_Model(nn.Module):\n","  def __init__(self, dim, num_hidden, bypass=True):\n","    super().__init__()\n","    self.bypass = bypass\n","    self.hidden = Linear(dim, num_hidden)\n","    if self.bypass:\n","      self.output = Linear(num_hidden + dim, out_dim)\n","    else:\n","      self.output = Linear(num_hidden, out_dim)\n","    self.BTU = BTU(0.5)\n","    self.y1 = 0\n","\n","  def forward(self, input):\n","    z1 = self.hidden(input)\n","    self.y1 = self.BTU(z1)\n","    if self.bypass:\n","      y1_concat = torch.cat((input, self.y1), 1)\n","      z2 = self.output(y1_concat)\n","    else:\n","      z2 = self.output(self.y1)\n","    return self.BTU(z2)\n","\n","  def show_net_data(self):\n","    print(\"\\n---------------Model parameters---------------:\")\n","    print(\"\\nhidden layer weights:\")\n","    print(self.hidden.weight)\n","    print(\"\\nhidden layer baises:\")\n","    print(self.hidden.bias)\n","    print(\"\\noutput layer weights:\")\n","    print(self.output.weight)\n","    print(\"\\noutput layer baises:\")\n","    print(self.output.bias)\n"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"c12Ukvx13z_-","executionInfo":{"status":"ok","timestamp":1669297645984,"user_tz":-120,"elapsed":514,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}}},"source":["def Loss(out, t_train):\n","  return -torch.sum(t_train * torch.log(out) + (1.0 - t_train) * torch.log(1.0 - out))  # Cross Entropy loss function\n"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpOI9Q0M4P_4","executionInfo":{"status":"ok","timestamp":1669297648344,"user_tz":-120,"elapsed":2,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}}},"source":["# def train func\n","def train(model, x_train, t_train, optimizer):\n","  y_pred = model(x_train)\n","  loss = Loss(y_pred, t_train)\n","  # zero gradients berfore running the backward pass\n","  optimizer.zero_grad()\n","  # backward pass to compute the gradient of loss\n","  # backprop + accumulate \n","  loss.backward()\n","  # update params\n","  optimizer.step()\n","  return loss"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7Jc7l1Q8pMD","executionInfo":{"status":"ok","timestamp":1669297650696,"user_tz":-120,"elapsed":4,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}}},"source":["# define test step operation:\n","def test(model, x_test, t_test):\n","  loss = Loss(model(x_test), t_test)\n","  return loss"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"dizmDqVl5TtJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669297935684,"user_tz":-120,"elapsed":283461,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"41ca2d41-095e-4ad8-97f3-af259d96319c"},"source":["# run for each possible trial values combinations (8 total diffent modle and each modle run 10 times)\n","model_count = 0\n","model_epochs_mean =[]\n","model_epochs_std =[]\n","run_number = 1\n","for l_rate in l_rate_options:\n","  for num_hidden in num_hidden_options:\n","    for bypass in bypass_options:\n","      model_count += 1\n","      print(\"--------------------------------------------------\\n--------------------MODEL NO. \"+ str(model_count) +\"-------------------\\n--------------------------------------------------\")\n","      print(\"Model values: l_rate = \"+ str(l_rate) +\", num_hidden = \"+ str(num_hidden) +\", bypass= \"+ str(bypass))\n","      success_cout = 0\n","      epochs_count = []\n","      train_losses_count = []\n","      validation_losses_count = []\n","      flag_not_10 = True\n","      run_number = 1\n","      while flag_not_10:\n","        print(\"------------------run No.\"+ str(run_number) +\"--------------------\")\n","        run_number += 1\n","        # create model and show initial values\n","        model = XOR_Net_Model(dim, num_hidden, bypass)\n","        model.show_net_data()\n","        # create methon for training\n","        optimizer = torch.optim.SGD(model.parameters(), lr=l_rate)\n","        # create list for keeping track of train loss values\n","        latest_v_lost = []\n","        # for each specific modle run trial.\n","        print(\"\\n----------Epochs----------\")\n","        for i in range(num_epocs): \n","          # train the model and get back the current loss value\n","          train_loss = train(model, x_train, t_train, optimizer)\n","          train_losses_count.append(train_loss)\n","          # Evaluate test accuracy\n","          validation_loss = test(model, x_validation, t_validation)\n","          validation_losses_count.append(validation_loss)\n","          latest_v_lost.append(train_loss)\n","          if len(latest_v_lost) > 10:\n","            latest_v_lost.pop(0)\n","          # print res once between 10000 runs\n","          if i % 10000 == 0:\n","            print(\"\\nEpoch: %s, loss: %s\" % (i, validation_loss))\n","            print(\"model output: \")\n","            print(model(x_train))\n","          # keep track of the last 10 loss values\n","          # stop procces condition, success\n","          if len(latest_v_lost) > 2 and abs(latest_v_lost[0] - latest_v_lost[-1]) <  0.0001 and train_loss < 0.2:\n","            success_cout += 1\n","            if success_cout == 10:\n","              flag_not_10 = False\n","            epochs_count.append(i)\n","            train_losses_count.append(train_loss)\n","            print(\"Success....... stoped at epocch number \" + str(i))\n","            break\n","          # anunoce failure\n","          if i == num_epocs-1:\n","            print(\"Failure.......\")\n","\n","      print(\"succesful run so far: \" + str(success_cout))\n","\n","  \n","      # convert list of tensors to match mean,sdtq inputs\n","      epochs_count = torch.FloatTensor(epochs_count)\n","      train_losses_count = torch.FloatTensor(train_losses_count)\n","      validation_losses_count = torch.FloatTensor(validation_losses_count)\n","      #cal means\n","      epochs_mean = torch.mean(epochs_count)\n","      model_epochs_mean.append(epochs_mean)\n","      train_losses_mean = torch.mean(train_losses_count)\n","      validation_losses_mean = torch.mean(validation_losses_count)\n","      # cal stdv's\n","      epochs_stdv = torch.std(epochs_count)\n","      model_epochs_std.append(epochs_stdv)\n","      train_losses_stdv = torch.std(train_losses_count)\n","      validation_losses_stdv = torch.std(validation_losses_count)\n","\n","      print(\"*** Successful runs  = \" + str(success_cout) + \" ***\")\n","      print(\"*** Failed runs  = \" + str(10 - success_cout) + \" ***\")\n","      print(\"*** Epochs for successful train runs:  Avg  = \" + str(epochs_mean) + \", Stdv = \" + str(epochs_stdv) +\" ***\")\n","      print(\"*** Train loss for successful train runs:  Avg  = \" + str(train_losses_mean) + \", Stdv = \" + str(train_losses_stdv) +\" ***\")\n","      print(\"*** validation loss for successful train runs:  Avg  = \" + str(validation_losses_mean) + \", Stdv = \" + str(validation_losses_stdv) +\" ***\")\n","\n","\n","\n"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","--------------------MODEL NO. 1-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.01, num_hidden = 2, bypass= False\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3384, 0.8548],\n","        [0.0118, 0.4950]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6657, 0.0297], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4473, 0.2126]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8798], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(11.3433, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9328],\n","        [0.9404],\n","        [0.9403],\n","        [0.9393]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4795\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7476, 0.3098],\n","        [0.0271, 0.1251]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9589, 0.2579], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0969, 0.2422]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5160], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.4098, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8032],\n","        [0.8065],\n","        [0.8063],\n","        [0.8047]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5493\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7696, 0.3921],\n","        [0.7054, 0.7152]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0042, 0.8079], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7799, 0.1558]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8017], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(12.2301, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9527],\n","        [0.9574],\n","        [0.9565],\n","        [0.9428]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4975\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3988, 0.3073],\n","        [0.2965, 0.5117]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2247, 0.6638], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5278, 0.7772]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9869], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(16.2324, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9816],\n","        [0.9843],\n","        [0.9841],\n","        [0.9816]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4858\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.4217, 0.1809],\n","        [0.2218, 0.7193]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6697, 0.3090], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5375, 0.7479]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4120], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(11.8983, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9401],\n","        [0.9518],\n","        [0.9513],\n","        [0.9457]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5280\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7513, 0.3018],\n","        [0.5856, 0.6976]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2970, 0.0439], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5725, 0.0290]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8045], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(10.5108, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9242],\n","        [0.9269],\n","        [0.9262],\n","        [0.9152]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4651\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7445, 0.1617],\n","        [0.4751, 0.6472]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5922, 0.4682], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4821, 0.8195]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5429], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(13.2003, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9619],\n","        [0.9663],\n","        [0.9658],\n","        [0.9595]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5281\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9677, 0.6329],\n","        [0.6457, 0.7695]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5348, 0.9659], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7564, 0.9141]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0315], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(12.4071, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9554],\n","        [0.9586],\n","        [0.9583],\n","        [0.9530]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5854\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1126, 0.2237],\n","        [0.3763, 0.5993]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7665, 0.9129], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.1865, 0.7440]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4025], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(10.1490, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9117],\n","        [0.9169],\n","        [0.9166],\n","        [0.9138]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5354\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3620, 0.7515],\n","        [0.9614, 0.2650]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1604, 0.2433], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.3690, 0.1991]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5666], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.8191, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8702],\n","        [0.8822],\n","        [0.8814],\n","        [0.8705]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4641\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(5118.2002), Stdv = tensor(398.4450) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.7032), Stdv = tensor(1.0041) ***\n","*** validation loss for successful train runs:  Avg  = tensor(5.3260), Stdv = tensor(0.6475) ***\n","--------------------------------------------------\n","--------------------MODEL NO. 2-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.01, num_hidden = 2, bypass= True\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2551, 0.6184],\n","        [0.0136, 0.4217]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1034, 0.8762], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2698, 0.7779, 0.7997, 0.3832]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.3894], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(15.9244, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9516],\n","        [0.9886],\n","        [0.9879],\n","        [0.9810]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 7929\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1434, 0.7609],\n","        [0.6555, 0.0906]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7226, 0.1247], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.3996, 0.0937, 0.7778, 0.1747]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7848], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(14.9752, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9770],\n","        [0.9817],\n","        [0.9802],\n","        [0.9627]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6892\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.8918, 0.7495],\n","        [0.2122, 0.1093]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1536, 0.1817], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9136, 0.5599, 0.2333, 0.9849]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4206], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(17.8439, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9862],\n","        [0.9945],\n","        [0.9933],\n","        [0.9686]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4117\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6833, 0.2316],\n","        [0.1456, 0.8124]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9182, 0.7144], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9501, 0.9839, 0.7163, 0.5025]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8311], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(25.5256, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9967],\n","        [0.9993],\n","        [0.9992],\n","        [0.9963]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6652\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2629, 0.2764],\n","        [0.8610, 0.3511]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.3768, 0.2826], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.3294, 0.7039, 0.7015, 0.8324]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5828], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(19.7233, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9867],\n","        [0.9960],\n","        [0.9957],\n","        [0.9906]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5041\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9090, 0.2723],\n","        [0.4500, 0.3917]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9642, 0.9833], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8638, 0.7465, 0.8595, 0.7261]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.3823], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(23.0702, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9955],\n","        [0.9986],\n","        [0.9983],\n","        [0.9930]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6587\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.4507, 0.7209],\n","        [0.0323, 0.4232]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.4015, 0.1255], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.3039, 0.3413, 0.4623, 0.2863]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7600], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(13.8496, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9562],\n","        [0.9764],\n","        [0.9751],\n","        [0.9606]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6194\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9896, 0.8397],\n","        [0.3943, 0.1649]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9545, 0.3004], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7869, 0.7560, 0.7341, 0.9854]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4535], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(23.0924, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9953],\n","        [0.9986],\n","        [0.9984],\n","        [0.9933]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4498\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9941, 0.4536],\n","        [0.0672, 0.7390]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9535, 0.9079], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8520, 0.0773, 0.5062, 0.4081]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9374], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(18.6393, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9934],\n","        [0.9942],\n","        [0.9931],\n","        [0.9742]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6561\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2501, 0.0951],\n","        [0.5445, 0.9149]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5533, 0.4753], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0925, 0.3394, 0.3697, 0.8866]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8848], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(17.3239, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9812],\n","        [0.9898],\n","        [0.9896],\n","        [0.9875]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5682\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(6015.2998), Stdv = tensor(1176.5719) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.3183), Stdv = tensor(0.5755) ***\n","*** validation loss for successful train runs:  Avg  = tensor(11.2208), Stdv = tensor(2.7840) ***\n","--------------------------------------------------\n","--------------------MODEL NO. 3-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.01, num_hidden = 4, bypass= False\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.4275, 0.6205],\n","        [0.7249, 0.1441],\n","        [0.3697, 0.9677],\n","        [0.3794, 0.4393]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1237, 0.1955, 0.2037, 0.9319], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4085, 0.7217, 0.6519, 0.7870]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.6928], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(22.0234, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9957],\n","        [0.9970],\n","        [0.9969],\n","        [0.9953]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4519\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.8897, 0.0170],\n","        [0.9316, 0.5634],\n","        [0.1740, 0.8641],\n","        [0.3657, 0.8622]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1796, 0.6305, 0.3614, 0.9063], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9267, 0.2900, 0.2740, 0.5352]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0446], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(13.5166, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9676],\n","        [0.9715],\n","        [0.9705],\n","        [0.9528]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4923\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7113, 0.9921],\n","        [0.4108, 0.7518],\n","        [0.3908, 0.9666],\n","        [0.0360, 0.4811]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9687, 0.6657, 0.6348, 0.2603], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.1226, 0.2667, 0.2132, 0.6088]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8264], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(13.7743, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9623],\n","        [0.9697],\n","        [0.9696],\n","        [0.9687]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4294\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5437, 0.4678],\n","        [0.5665, 0.7342],\n","        [0.8648, 0.0301],\n","        [0.7134, 0.0803]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6788, 0.2233, 0.5696, 0.2044], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4757, 0.1854, 0.8297, 0.5497]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.1826], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(14.6648, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9769],\n","        [0.9786],\n","        [0.9778],\n","        [0.9641]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4638\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3770, 0.5665],\n","        [0.8586, 0.2084],\n","        [0.1768, 0.9426],\n","        [0.0178, 0.8608]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7692, 0.3263, 0.8900, 0.4849], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8218, 0.8158, 0.3017, 0.8144]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9343], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(25.8382, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9982],\n","        [0.9988],\n","        [0.9988],\n","        [0.9983]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5526\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1624, 0.6118],\n","        [0.1346, 0.2519],\n","        [0.0369, 0.7046],\n","        [0.7204, 0.2673]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8039, 0.2678, 0.1264, 0.7019], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.1792, 0.8842, 0.7774, 0.8941]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5641], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(21.1504, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9933],\n","        [0.9960],\n","        [0.9959],\n","        [0.9949]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4299\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5827, 0.8991],\n","        [0.6579, 0.8691],\n","        [0.6440, 0.2638],\n","        [0.4277, 0.3199]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0273, 0.6636, 0.0094, 0.2581], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.3641, 0.6864, 0.2956, 0.4153]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.3873], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(14.1852, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9701],\n","        [0.9761],\n","        [0.9756],\n","        [0.9683]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4279\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3778, 0.2836],\n","        [0.3026, 0.2604],\n","        [0.8361, 0.6287],\n","        [0.9146, 0.4599]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7563, 0.6347, 0.5873, 0.7738], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.1492, 0.2677, 0.4519, 0.5979]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5791], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(14.3222, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9725],\n","        [0.9743],\n","        [0.9741],\n","        [0.9702]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4847\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.0557, 0.4278],\n","        [0.5540, 0.8793],\n","        [0.5381, 0.9653],\n","        [0.6707, 0.4580]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1269, 0.9877, 0.7707, 0.9154], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.6838, 0.6997, 0.8925, 0.9137]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4044], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(24.8090, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9977],\n","        [0.9984],\n","        [0.9983],\n","        [0.9981]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4932\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.8899, 0.1109],\n","        [0.7970, 0.8032],\n","        [0.4274, 0.6318],\n","        [0.4260, 0.9402]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7640, 0.2897, 0.9855, 0.5705], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4505, 0.0639, 0.6109, 0.6502]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8014], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(18.3197, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9896],\n","        [0.9909],\n","        [0.9908],\n","        [0.9895]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4603\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(4686.), Stdv = tensor(387.9247) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.6114), Stdv = tensor(0.9643) ***\n","*** validation loss for successful train runs:  Avg  = tensor(6.4490), Stdv = tensor(1.4834) ***\n","--------------------------------------------------\n","--------------------MODEL NO. 4-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.01, num_hidden = 4, bypass= True\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5417, 0.5949],\n","        [0.9772, 0.3844],\n","        [0.4052, 0.7412],\n","        [0.9500, 0.9477]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8603, 0.5110, 0.3275, 0.4157], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4810, 0.2071, 0.9100, 0.5952, 0.6605, 0.0623]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.1045], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(19.3277, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9922],\n","        [0.9952],\n","        [0.9947],\n","        [0.9866]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6301\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5421, 0.3036],\n","        [0.1050, 0.4844],\n","        [0.6978, 0.2944],\n","        [0.1289, 0.7757]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5191, 0.8361, 0.8401, 0.1212], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4536, 0.0228, 0.7483, 0.1519, 0.2327, 0.1987]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7840], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(16.8416, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9877],\n","        [0.9891],\n","        [0.9880],\n","        [0.9733]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6414\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1540, 0.7402],\n","        [0.1115, 0.1525],\n","        [0.6962, 0.9388],\n","        [0.7270, 0.8362]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7951, 0.9122, 0.4338, 0.8296], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2405, 0.3388, 0.9223, 0.2868, 0.5407, 0.4801]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4961], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(21.9573, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9943],\n","        [0.9972],\n","        [0.9971],\n","        [0.9956]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6016\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2082, 0.7113],\n","        [0.7682, 0.3469],\n","        [0.0425, 0.3934],\n","        [0.9975, 0.7778]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9354, 0.3871, 0.3551, 0.2974], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8085, 0.7201, 0.7648, 0.8116, 0.7324, 0.9363]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8056], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(36.1667, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9998],\n","        [1.0000],\n","        [0.9999],\n","        [0.9998]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5276\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9870, 0.6447],\n","        [0.2803, 0.2486],\n","        [0.4827, 0.8404],\n","        [0.8653, 0.4876]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5158, 0.6649, 0.6813, 0.8840], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5468, 0.4919, 0.9876, 0.1200, 0.7333, 0.6208]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8146], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(28.9187, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9991],\n","        [0.9996],\n","        [0.9996],\n","        [0.9989]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6342\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6333, 0.8911],\n","        [0.6270, 0.9607],\n","        [0.1043, 0.4922],\n","        [0.9167, 0.9795]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2722, 0.9173, 0.9006, 0.7448], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0535, 0.9101, 0.8177, 0.0419, 0.0056, 0.7875]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5912], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(20.4695, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9834],\n","        [0.9965],\n","        [0.9964],\n","        [0.9959]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6330\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.0997, 0.1196],\n","        [0.9956, 0.2451],\n","        [0.7036, 0.7411],\n","        [0.0871, 0.0919]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0066, 0.2194, 0.8320, 0.2821], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8041, 0.5006, 0.4333, 0.0963, 0.5738, 0.1997]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.6883], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(19.6476, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9914],\n","        [0.9962],\n","        [0.9955],\n","        [0.9835]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4730\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9016, 0.9133],\n","        [0.4742, 0.1117],\n","        [0.9469, 0.6166],\n","        [0.9233, 0.6718]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6030, 0.4901, 0.8950, 0.0858], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.1023, 0.0125, 0.3622, 0.1666, 0.8885, 0.7495]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9668], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(22.3721, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9969],\n","        [0.9972],\n","        [0.9971],\n","        [0.9957]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 6321\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1232, 0.3557],\n","        [0.0280, 0.2439],\n","        [0.2846, 0.9714],\n","        [0.0792, 0.0385]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5492, 0.2246, 0.6458, 0.9521], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.3979, 0.6232, 0.2310, 0.3740, 0.8979, 0.8953]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.1821], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(22.2555, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9927],\n","        [0.9978],\n","        [0.9976],\n","        [0.9954]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 4184\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.0922, 0.8398],\n","        [0.4153, 0.8290],\n","        [0.9741, 0.5915],\n","        [0.9575, 0.7490]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0137, 0.2881, 0.6439, 0.9015], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9978, 0.8838, 0.9731, 0.6682, 0.2636, 0.2837]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7739], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(29.8753, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9987],\n","        [0.9998],\n","        [0.9998],\n","        [0.9988]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 5657\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(5757.1001), Stdv = tensor(785.5455) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.3134), Stdv = tensor(0.6237) ***\n","*** validation loss for successful train runs:  Avg  = tensor(11.1843), Stdv = tensor(2.7728) ***\n","--------------------------------------------------\n","--------------------MODEL NO. 5-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.1, num_hidden = 2, bypass= False\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2617, 0.3473],\n","        [0.5713, 0.7148]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6060, 0.9083], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5410, 0.9181]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.3943], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.4042, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8035],\n","        [0.8120],\n","        [0.8114],\n","        [0.8050]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1289\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9478, 0.9683],\n","        [0.9493, 0.5216]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1980, 0.4583], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2435, 0.8483]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8949], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.0047, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8421],\n","        [0.8437],\n","        [0.8432],\n","        [0.8340]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1231\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.0715, 0.1726],\n","        [0.4564, 0.3009]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.4382, 0.0012], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4595, 0.1335]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8210], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.5379, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7338],\n","        [0.7319],\n","        [0.7327],\n","        [0.7403]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1099\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2924, 0.1515],\n","        [0.8731, 0.0908]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9597, 0.7834], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5861, 0.5245]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7820], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.6628, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8218],\n","        [0.8230],\n","        [0.8225],\n","        [0.8157]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1253\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.8373, 0.9506],\n","        [0.3963, 0.8504]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2830, 0.3463], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.6153, 0.0211]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5226], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.6859, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6114],\n","        [0.6046],\n","        [0.6046],\n","        [0.6019]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1238\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2421, 0.1255],\n","        [0.1270, 0.0558]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7027, 0.6293], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5512, 0.1818]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.3616], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.7807, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6175],\n","        [0.6195],\n","        [0.6190],\n","        [0.6142]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1065\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.4708, 0.4156],\n","        [0.8018, 0.1224]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2937, 0.3888], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0638, 0.3170]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9056], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.0492, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6750],\n","        [0.6671],\n","        [0.6680],\n","        [0.6778]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1245\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2275, 0.4675],\n","        [0.5405, 0.2573]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0487, 0.3780], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8901, 0.9749]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7448], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(10.5077, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9149],\n","        [0.9298],\n","        [0.9285],\n","        [0.9143]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1045\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7402, 0.5675],\n","        [0.5506, 0.5862]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1666, 0.9533], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5966, 0.0963]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9324], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.6166, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7427],\n","        [0.7473],\n","        [0.7467],\n","        [0.7374]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1266\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2886, 0.4928],\n","        [0.1661, 0.3258]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5515, 0.5166], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2350, 0.6207]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2528], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.7195, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.5909],\n","        [0.6009],\n","        [0.6004],\n","        [0.5966]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1036\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(1176.7000), Stdv = tensor(101.8900) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.2789), Stdv = tensor(0.7047) ***\n","*** validation loss for successful train runs:  Avg  = tensor(6.8759), Stdv = tensor(1.4971) ***\n","--------------------------------------------------\n","--------------------MODEL NO. 6-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.1, num_hidden = 2, bypass= True\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2454, 0.3700],\n","        [0.4877, 0.6655]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6933, 0.5217], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2943, 0.2383, 0.5770, 0.3139]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2342], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.4492, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.5001],\n","        [0.4688],\n","        [0.4712],\n","        [0.4901]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 2042\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2752, 0.0717],\n","        [0.1399, 0.0418]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8937, 0.5323], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9098, 0.3855, 0.6003, 0.7245]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2181], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.9583, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8542],\n","        [0.8598],\n","        [0.8459],\n","        [0.6929]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1048\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.0775, 0.2455],\n","        [0.3155, 0.3738]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9390, 0.2878], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9991, 0.8338, 0.1099, 0.5465]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4219], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.3739, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7826],\n","        [0.8879],\n","        [0.8745],\n","        [0.7144]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1220\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1803, 0.2097],\n","        [0.8931, 0.4779]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5882, 0.2186], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7404, 0.1266, 0.8539, 0.8140]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8951], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(11.5769, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9626],\n","        [0.9506],\n","        [0.9465],\n","        [0.8949]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1203\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1080, 0.3260],\n","        [0.9750, 0.8595]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9434, 0.9083], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7115, 0.9701, 0.8827, 0.7720]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4549], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(12.9098, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9265],\n","        [0.9725],\n","        [0.9706],\n","        [0.9497]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1285\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5342, 0.9907],\n","        [0.8100, 0.3372]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1817, 0.4181], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8314, 0.5576, 0.4822, 0.7328]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7717], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(10.7737, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9252],\n","        [0.9470],\n","        [0.9419],\n","        [0.8768]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 2049\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.4591, 0.7189],\n","        [0.2853, 0.0141]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9018, 0.4058], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.1508, 0.6118, 0.6611, 0.3192]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0482], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.7225, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.4210],\n","        [0.5361],\n","        [0.5454],\n","        [0.6172]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1284\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.9284, 0.7220],\n","        [0.8023, 0.5472]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.3677, 0.4177], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.4403, 0.5756, 0.8629, 0.0272]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8381], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.7664, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7867],\n","        [0.8414],\n","        [0.8396],\n","        [0.8231]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 2074\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7269, 0.9379],\n","        [0.6129, 0.3220]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0236, 0.9707], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9637, 0.7301, 0.0866, 0.0109]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7560], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.1413, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6955],\n","        [0.7954],\n","        [0.7764],\n","        [0.5929]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1526\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3094, 0.3042],\n","        [0.4205, 0.5333]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0405, 0.2383], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7581, 0.0699, 0.7665, 0.7004]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.3032], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.0773, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8382],\n","        [0.7941],\n","        [0.7787],\n","        [0.6248]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1229\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(1496.), Stdv = tensor(403.0980) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.1436), Stdv = tensor(0.4382) ***\n","*** validation loss for successful train runs:  Avg  = tensor(inf), Stdv = tensor(nan) ***\n","--------------------------------------------------\n","--------------------MODEL NO. 7-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.1, num_hidden = 4, bypass= False\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5696, 0.5975],\n","        [0.3240, 0.7081],\n","        [0.9000, 0.8879],\n","        [0.4422, 0.3806]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7228, 0.7504, 0.0174, 0.2392], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9039, 0.9534, 0.0318, 0.0691]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2415], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.0652, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6643],\n","        [0.6674],\n","        [0.6681],\n","        [0.6806]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1072\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6364, 0.5233],\n","        [0.4754, 0.0589],\n","        [0.7247, 0.3336],\n","        [0.5888, 0.4721]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8040, 0.0896, 0.6518, 0.6990], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0740, 0.9769, 0.0952, 0.6529]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2279], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.6309, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.5202],\n","        [0.5107],\n","        [0.5055],\n","        [0.4665]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1118\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7333, 0.3497],\n","        [0.9261, 0.7332],\n","        [0.8481, 0.6689],\n","        [0.7009, 0.6375]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.4025, 0.1505, 0.4040, 0.8103], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0977, 0.5778, 0.7063, 0.2651]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5244], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.7978, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6316],\n","        [0.6385],\n","        [0.6383],\n","        [0.6323]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1186\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1042, 0.7223],\n","        [0.5892, 0.6764],\n","        [0.9103, 0.4844],\n","        [0.0738, 0.7664]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7758, 0.3954, 0.7927, 0.3998], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5351, 0.0697, 0.5363, 0.7528]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.6182], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.8207, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7373],\n","        [0.7615],\n","        [0.7617],\n","        [0.7648]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1224\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7804, 0.5192],\n","        [0.0940, 0.6380],\n","        [0.4024, 0.5048],\n","        [0.8025, 0.6317]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7743, 0.3424, 0.6376, 0.3840], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.6254, 0.3190, 0.7909, 0.6935]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.1132], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.8326, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7642],\n","        [0.7783],\n","        [0.7770],\n","        [0.7573]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1233\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5214, 0.5375],\n","        [0.4804, 0.2850],\n","        [0.9382, 0.5846],\n","        [0.5547, 0.8063]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8704, 0.1868, 0.1016, 0.2794], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0143, 0.0470, 0.7437, 0.9279]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5846], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.2395, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6985],\n","        [0.7199],\n","        [0.7191],\n","        [0.7011]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1224\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.8883, 0.9089],\n","        [0.8245, 0.5281],\n","        [0.5760, 0.5071],\n","        [0.3258, 0.0249]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7784, 0.8523, 0.8347, 0.8689], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0014, 0.0650, 0.5808, 0.3154]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0440], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.3234, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.2914],\n","        [0.2905],\n","        [0.2905],\n","        [0.2919]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1050\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3121, 0.8057],\n","        [0.0577, 0.3436],\n","        [0.5335, 0.5382],\n","        [0.5916, 0.4996]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.4394, 0.2663, 0.2490, 0.2822], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8118, 0.7475, 0.6797, 0.8087]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0051], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.5666, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8455],\n","        [0.8818],\n","        [0.8803],\n","        [0.8615]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1062\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7475, 0.6160],\n","        [0.8860, 0.9271],\n","        [0.7243, 0.7267],\n","        [0.5624, 0.4282]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.9315, 0.6030, 0.3933, 0.9489], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7054, 0.4730, 0.3489, 0.1908]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0669], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.5518, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.4646],\n","        [0.4670],\n","        [0.4669],\n","        [0.4637]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1248\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5276, 0.4147],\n","        [0.5464, 0.9020],\n","        [0.3679, 0.5925],\n","        [0.5724, 0.8563]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7476, 0.3973, 0.6119, 0.3078], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2857, 0.4174, 0.6672, 0.0524]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.3403], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.5648, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.4775],\n","        [0.4753],\n","        [0.4754],\n","        [0.4785]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1225\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(1164.2000), Stdv = tensor(79.7229) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.2515), Stdv = tensor(0.6789) ***\n","*** validation loss for successful train runs:  Avg  = tensor(6.9458), Stdv = tensor(1.7048) ***\n","--------------------------------------------------\n","--------------------MODEL NO. 8-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.1, num_hidden = 4, bypass= True\n","------------------run No.1--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1785, 0.3124],\n","        [0.1304, 0.8359],\n","        [0.4237, 0.3477],\n","        [0.9984, 0.3162]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2358, 0.6607, 0.5892, 0.9081], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0946, 0.0979, 0.4988, 0.6314, 0.3110, 0.2436]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8904], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.6658, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.6587],\n","        [0.5803],\n","        [0.5937],\n","        [0.6973]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1464\n","------------------run No.2--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3859, 0.6452],\n","        [0.0257, 0.0813],\n","        [0.2553, 0.3297],\n","        [0.5878, 0.7676]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1244, 0.0635, 0.9175, 0.7162], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.6333, 0.2939, 0.3759, 0.8787, 0.0972, 0.8457]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2979], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.5065, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7635],\n","        [0.7434],\n","        [0.7349],\n","        [0.6591]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1159\n","------------------run No.3--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.4165, 0.7237],\n","        [0.9756, 0.4005],\n","        [0.4736, 0.1579],\n","        [0.9599, 0.4899]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6081, 0.9943, 0.7462, 0.3865], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.3854, 0.6012, 0.1044, 0.0516, 0.6736, 0.6192]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.5765], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.0189, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.5641],\n","        [0.6524],\n","        [0.6512],\n","        [0.6373]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1574\n","------------------run No.4--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6899, 0.6445],\n","        [0.2108, 0.6048],\n","        [0.5613, 0.5052],\n","        [0.5883, 0.9537]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7877, 0.9042, 0.8631, 0.6099], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.6579, 0.7292, 0.3791, 0.5728, 0.8440, 0.7383]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8658], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(14.9521, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9669],\n","        [0.9829],\n","        [0.9818],\n","        [0.9704]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1951\n","------------------run No.5--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.0890, 0.4707],\n","        [0.1593, 0.6840],\n","        [0.0258, 0.2587],\n","        [0.4620, 0.7058]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2593, 0.5424, 0.1017, 0.5090], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2293, 0.8454, 0.8948, 0.1231, 0.9895, 0.7673]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2070], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.6311, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7188],\n","        [0.8733],\n","        [0.8775],\n","        [0.9059]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1062\n","------------------run No.6--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1259, 0.0012],\n","        [0.0529, 0.8127],\n","        [0.0598, 0.3419],\n","        [0.1444, 0.3992]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0468, 0.6071, 0.6475, 0.8699], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5721, 0.2490, 0.3218, 0.6661, 0.3310, 0.8707]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7252], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(9.8412, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9175],\n","        [0.9134],\n","        [0.9101],\n","        [0.8795]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1209\n","------------------run No.7--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[3.4783e-01, 2.7765e-01],\n","        [1.1049e-01, 5.5579e-01],\n","        [3.7491e-01, 4.1984e-01],\n","        [6.4972e-01, 4.9490e-04]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.1423, 0.3979, 0.6325, 0.2632], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8346, 0.9499, 0.9934, 0.7511, 0.4610, 0.6114]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.6519], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(16.4768, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9718],\n","        [0.9908],\n","        [0.9897],\n","        [0.9733]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1148\n","------------------run No.8--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.0039, 0.9409],\n","        [0.2492, 0.4410],\n","        [0.8802, 0.2546],\n","        [0.1158, 0.0188]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.3517, 0.0347, 0.0419, 0.3111], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.8948, 0.8697, 0.5644, 0.3471, 0.4077, 0.3564]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0424], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.9754, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7194],\n","        [0.8656],\n","        [0.8528],\n","        [0.7104]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1184\n","------------------run No.9--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2088, 0.1290],\n","        [0.6036, 0.1761],\n","        [0.5849, 0.6122],\n","        [0.0592, 0.9618]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8304, 0.5251, 0.7908, 0.2299], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.2753, 0.2840, 0.3476, 0.4968, 0.0445, 0.3407]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8357], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.4863, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.5588],\n","        [0.5244],\n","        [0.5292],\n","        [0.5671]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1381\n","------------------run No.10--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.8174, 0.9348],\n","        [0.9017, 0.9107],\n","        [0.6640, 0.6781],\n","        [0.5010, 0.8536]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2489, 0.1150, 0.7690, 0.5635], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9671, 0.6084, 0.8442, 0.4600, 0.3994, 0.5555]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.1873], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(9.5146, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8880],\n","        [0.9288],\n","        [0.9202],\n","        [0.8056]], grad_fn=<MulBackward0>)\n","Success....... stoped at epocch number 1956\n","succesful run so far: 10\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(1408.8000), Stdv = tensor(327.5603) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.1432), Stdv = tensor(0.5145) ***\n","*** validation loss for successful train runs:  Avg  = tensor(inf), Stdv = tensor(nan) ***\n"]}]},{"cell_type":"code","source":["# the 9th model\n","print(\"--------------------------------------------------\\n--------------------MODEL NO.9-------------------\\n--------------------------------------------------\")\n","print(\"Model values: l_rate = 0.01, num_hidden = 1, bypass= True\")\n","success_cout = 0\n","epochs_count = []\n","train_losses_count = []\n","validation_losses_count = []\n","flag_not_10 = True\n","while flag_not_10:\n","  print(\"------------------run No.\"+ str(i+1) +\"--------------------\")\n","  # create model and show initial values\n","  model = XOR_Net_Model(dim, 1, True)\n","  model.show_net_data()\n","  # create methon for training\n","  optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","  # create list for keeping track of train loss values\n","  latest_v_lost = []\n","  # for each specific modle run trial.\n","  print(\"\\n----------Epochs----------\")\n","  for i in range(num_epocs): \n","    # train the model and get back the current loss value\n","    train_loss = train(model, x_train, t_train, optimizer)\n","    train_losses_count.append(train_loss)\n","    # print res once between 1000 runs\n","    if i % 1000 == 0:\n","      print(\"\\nEpoch: %s, loss: %s\" % (i, train_loss))\n","      print(\"model output: \")\n","      print(model(x_train))\n","      print(\"---------------------\")\n","      print(\"Neuron inputs: \")\n","      print(x_train)\n","      print(\"Neuron outputs: \")\n","      print(model.y1)\n","      print(\"---------------------\")\n","      # after a few runs it seems that the Neuron sometimes acts like an OR Gate and sometimes as XOR Gate\n","\n","      # Evaluate test accuracy\n","    validation_loss = test(model, x_validation, t_validation)\n","    validation_losses_count.append(validation_loss)\n","  \n","    # keep track of the last 10 loss values\n","    latest_v_lost.append(train_loss)\n","    if len(latest_v_lost) > 10:\n","      latest_v_lost.pop(0)\n","    # stop procces condition, success\n","    if len(latest_v_lost) > 2 and abs(latest_v_lost[0] - latest_v_lost[-1]) <  0.0001 and train_loss < 0.2:\n","      success_cout += 1\n","      if success_cout == 10:\n","        flag_not_10 = False\n","      epochs_count.append(i)\n","      train_losses_count.append(train_loss)\n","      print(\"Success....... stoped at epocch number \" + str(i))\n","      break\n","    # anunoce failure\n","    if i == num_epocs-1:\n","      print(\"Failure.......\")\n","\n","\n","\n","# convert list of tensors to match mean,sdtq inputs\n","epochs_count = torch.FloatTensor(epochs_count)\n","train_losses_count = torch.FloatTensor(train_losses_count)\n","validation_losses_count = torch.FloatTensor(validation_losses_count)\n","#cal means\n","epochs_mean = torch.mean(epochs_count)\n","model_epochs_mean.append(epochs_mean)\n","train_losses_mean = torch.mean(train_losses_count)\n","validation_losses_mean = torch.mean(validation_losses_count)\n","# cal stdv's\n","epochs_stdv = torch.std(epochs_count)\n","model_epochs_std.append(epochs_stdv)\n","train_losses_stdv = torch.std(train_losses_count)\n","validation_losses_stdv = torch.std(validation_losses_count)\n","\n","print(\"*** Successful runs  = \" + str(success_cout) + \" ***\")\n","print(\"*** Failed runs  = \" + str(10 - success_cout) + \" ***\")\n","print(\"*** Epochs for successful train runs:  Avg  = \" + str(epochs_mean) + \", Stdv = \" + str(epochs_stdv) +\" ***\")\n","print(\"*** Train loss for successful train runs:  Avg  = \" + str(train_losses_mean) + \", Stdv = \" + str(train_losses_stdv) +\" ***\")\n","print(\"*** validation loss for successful train runs:  Avg  = \" + str(validation_losses_mean) + \", Stdv = \" + str(validation_losses_stdv) +\" ***\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOkzRWGhuX2U","executionInfo":{"status":"ok","timestamp":1669298078599,"user_tz":-120,"elapsed":66582,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"39c19dfe-6e4a-4fed-ec05-fa60760fe271"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","--------------------MODEL NO.9-------------------\n","--------------------------------------------------\n","Model values: l_rate = 0.01, num_hidden = 1, bypass= True\n","------------------run No.1957--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.7050, 0.2017]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.4810], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7444, 0.5161, 0.2459]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8852], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(9.2272, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9731],\n","        [0.9875],\n","        [0.9856],\n","        [0.9544]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9174],\n","        [0.9387],\n","        [0.9301],\n","        [0.8118]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5776, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8505],\n","        [0.0969],\n","        [0.1441],\n","        [0.8542]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9572],\n","        [0.9749],\n","        [0.9711],\n","        [0.9132]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.2824, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9244],\n","        [0.0438],\n","        [0.0791],\n","        [0.9264]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9784],\n","        [0.9899],\n","        [0.9882],\n","        [0.9606]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1839, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9504],\n","        [0.0268],\n","        [0.0543],\n","        [0.9514]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9850],\n","        [0.9937],\n","        [0.9926],\n","        [0.9739]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1357, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9632],\n","        [0.0189],\n","        [0.0414],\n","        [0.9638]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9881],\n","        [0.9954],\n","        [0.9945],\n","        [0.9798]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1074, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9709],\n","        [0.0144],\n","        [0.0335],\n","        [0.9712]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9899],\n","        [0.9963],\n","        [0.9956],\n","        [0.9832]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0888, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9759],\n","        [0.0115],\n","        [0.0282],\n","        [0.9761]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9911],\n","        [0.9969],\n","        [0.9963],\n","        [0.9854]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 7000, loss: tensor(0.0757, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9794],\n","        [0.0096],\n","        [0.0244],\n","        [0.9796]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9920],\n","        [0.9972],\n","        [0.9967],\n","        [0.9870]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 7032\n","------------------run No.7033--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.5496, 0.9591]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8441], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7005, 0.8356, 0.0171]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.6462], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.4574, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9351],\n","        [0.9811],\n","        [0.9785],\n","        [0.9401]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9516],\n","        [0.9892],\n","        [0.9879],\n","        [0.9714]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5679, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8530],\n","        [0.0958],\n","        [0.1425],\n","        [0.8574]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9720],\n","        [0.9943],\n","        [0.9935],\n","        [0.9817]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.2789, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9257],\n","        [0.0434],\n","        [0.0783],\n","        [0.9271]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9838],\n","        [0.9970],\n","        [0.9965],\n","        [0.9884]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1823, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9510],\n","        [0.0267],\n","        [0.0539],\n","        [0.9516]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9879],\n","        [0.9979],\n","        [0.9975],\n","        [0.9910]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1349, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9636],\n","        [0.0188],\n","        [0.0412],\n","        [0.9640]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9901],\n","        [0.9983],\n","        [0.9980],\n","        [0.9924]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1069, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9711],\n","        [0.0144],\n","        [0.0334],\n","        [0.9713]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9914],\n","        [0.9986],\n","        [0.9983],\n","        [0.9933]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0885, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9761],\n","        [0.0115],\n","        [0.0281],\n","        [0.9762]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9923],\n","        [0.9987],\n","        [0.9985],\n","        [0.9939]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 7000, loss: tensor(0.0754, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9796],\n","        [0.0096],\n","        [0.0243],\n","        [0.9796]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9930],\n","        [0.9989],\n","        [0.9987],\n","        [0.9944]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 7006\n","------------------run No.7007--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.3356, 0.4386]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0563], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9460, 0.3371, 0.8858]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.7328], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(10.7046, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9885],\n","        [0.9942],\n","        [0.9930],\n","        [0.9657]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.6965],\n","        [0.8202],\n","        [0.8104],\n","        [0.7173]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.4207, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8888],\n","        [0.0684],\n","        [0.1096],\n","        [0.8911]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.5755],\n","        [0.3337],\n","        [0.3630],\n","        [0.6162]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.1372, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9637],\n","        [0.0201],\n","        [0.0417],\n","        [0.9634]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.6645],\n","        [0.2421],\n","        [0.2830],\n","        [0.6821]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.0720, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9810],\n","        [0.0099],\n","        [0.0231],\n","        [0.9807]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.7065],\n","        [0.2058],\n","        [0.2503],\n","        [0.7167]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.0468, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9877],\n","        [0.0061],\n","        [0.0155],\n","        [0.9875]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.7306],\n","        [0.1852],\n","        [0.2311],\n","        [0.7374]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 4719\n","------------------run No.4720--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.8077, 0.1567]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5514], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.5425, 0.6592, 0.0225]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9336], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.3691, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9480],\n","        [0.9802],\n","        [0.9781],\n","        [0.9523]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9399],\n","        [0.9526],\n","        [0.9447],\n","        [0.8243]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5897, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8479],\n","        [0.0988],\n","        [0.1462],\n","        [0.8504]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9567],\n","        [0.9705],\n","        [0.9655],\n","        [0.8827]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.2869, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9231],\n","        [0.0445],\n","        [0.0802],\n","        [0.9253]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9780],\n","        [0.9886],\n","        [0.9865],\n","        [0.9492]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1860, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9497],\n","        [0.0272],\n","        [0.0549],\n","        [0.9509]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9848],\n","        [0.9931],\n","        [0.9918],\n","        [0.9674]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1369, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9629],\n","        [0.0191],\n","        [0.0418],\n","        [0.9636]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9880],\n","        [0.9950],\n","        [0.9940],\n","        [0.9754]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1082, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9706],\n","        [0.0145],\n","        [0.0338],\n","        [0.9711]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9899],\n","        [0.9960],\n","        [0.9952],\n","        [0.9798]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0894, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9757],\n","        [0.0116],\n","        [0.0284],\n","        [0.9760]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9912],\n","        [0.9967],\n","        [0.9960],\n","        [0.9827]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 7000, loss: tensor(0.0761, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9793],\n","        [0.0096],\n","        [0.0245],\n","        [0.9795]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9920],\n","        [0.9971],\n","        [0.9965],\n","        [0.9847]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 7068\n","------------------run No.7069--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6440, 0.9678]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.8407], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.6007, 0.5652, 0.0733]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.0246], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(5.3235, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.7900],\n","        [0.8990],\n","        [0.8881],\n","        [0.7610]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9594],\n","        [0.9911],\n","        [0.9899],\n","        [0.9721]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5623, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8546],\n","        [0.0949],\n","        [0.1416],\n","        [0.8588]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9827],\n","        [0.9967],\n","        [0.9962],\n","        [0.9865]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.2768, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9263],\n","        [0.0430],\n","        [0.0778],\n","        [0.9277]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9893],\n","        [0.9982],\n","        [0.9978],\n","        [0.9911]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1812, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9513],\n","        [0.0265],\n","        [0.0537],\n","        [0.9520]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9917],\n","        [0.9986],\n","        [0.9984],\n","        [0.9930]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1343, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9638],\n","        [0.0187],\n","        [0.0410],\n","        [0.9641]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9930],\n","        [0.9989],\n","        [0.9987],\n","        [0.9939]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1065, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9712],\n","        [0.0143],\n","        [0.0333],\n","        [0.9714]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9938],\n","        [0.9991],\n","        [0.9988],\n","        [0.9946]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0882, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9761],\n","        [0.0115],\n","        [0.0280],\n","        [0.9763]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9944],\n","        [0.9992],\n","        [0.9990],\n","        [0.9950]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 6986\n","------------------run No.6987--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6029, 0.9341]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.5612], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.9928, 0.3896, 0.5414]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.1693], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(8.1455, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9602],\n","        [0.9784],\n","        [0.9739],\n","        [0.8866]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9250],\n","        [0.9821],\n","        [0.9799],\n","        [0.9489]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5568, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8563],\n","        [0.0938],\n","        [0.1404],\n","        [0.8596]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9809],\n","        [0.9965],\n","        [0.9959],\n","        [0.9846]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.2750, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9268],\n","        [0.0427],\n","        [0.0774],\n","        [0.9281]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9890],\n","        [0.9982],\n","        [0.9979],\n","        [0.9906]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1804, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9515],\n","        [0.0263],\n","        [0.0534],\n","        [0.9522]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9917],\n","        [0.9987],\n","        [0.9985],\n","        [0.9927]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1337, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9639],\n","        [0.0186],\n","        [0.0409],\n","        [0.9643]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9930],\n","        [0.9990],\n","        [0.9988],\n","        [0.9938]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1061, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9713],\n","        [0.0142],\n","        [0.0332],\n","        [0.9715]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9939],\n","        [0.9991],\n","        [0.9989],\n","        [0.9945]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0879, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9762],\n","        [0.0114],\n","        [0.0280],\n","        [0.9763]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9945],\n","        [0.9992],\n","        [0.9991],\n","        [0.9950]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 6976\n","------------------run No.6977--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.2873, 0.1846]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.0006], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.1212, 0.6327, 0.3440]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.8760], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.3774, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9152],\n","        [0.9669],\n","        [0.9661],\n","        [0.9589]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.6437],\n","        [0.7066],\n","        [0.6948],\n","        [0.5917]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5449, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8572],\n","        [0.0891],\n","        [0.1340],\n","        [0.8581]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.6317],\n","        [0.5258],\n","        [0.5330],\n","        [0.5896]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.1996, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9465],\n","        [0.0297],\n","        [0.0578],\n","        [0.9466]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.6242],\n","        [0.3142],\n","        [0.3429],\n","        [0.5959]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1015, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9728],\n","        [0.0140],\n","        [0.0317],\n","        [0.9729]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.6678],\n","        [0.2587],\n","        [0.2953],\n","        [0.6443]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.0632, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9831],\n","        [0.0082],\n","        [0.0206],\n","        [0.9831]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.6968],\n","        [0.2289],\n","        [0.2694],\n","        [0.6758]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.0444, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9881],\n","        [0.0056],\n","        [0.0149],\n","        [0.9882]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.7165],\n","        [0.2095],\n","        [0.2520],\n","        [0.6971]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 5415\n","------------------run No.5416--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6964, 0.3427]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.7368], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.0857, 0.6928, 0.6437]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.2107], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(6.4340, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8515],\n","        [0.9441],\n","        [0.9433],\n","        [0.9341]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9493],\n","        [0.9700],\n","        [0.9657],\n","        [0.9024]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5366, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8602],\n","        [0.0900],\n","        [0.1363],\n","        [0.8653]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9849],\n","        [0.9937],\n","        [0.9926],\n","        [0.9743]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.2696, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9279],\n","        [0.0417],\n","        [0.0761],\n","        [0.9296]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9907],\n","        [0.9966],\n","        [0.9960],\n","        [0.9849]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1780, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9521],\n","        [0.0259],\n","        [0.0528],\n","        [0.9529]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9928],\n","        [0.9976],\n","        [0.9971],\n","        [0.9886]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1324, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9642],\n","        [0.0184],\n","        [0.0405],\n","        [0.9647]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9939],\n","        [0.9981],\n","        [0.9977],\n","        [0.9905]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1053, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9715],\n","        [0.0141],\n","        [0.0330],\n","        [0.9718]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9946],\n","        [0.9983],\n","        [0.9980],\n","        [0.9917]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0873, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9763],\n","        [0.0113],\n","        [0.0278],\n","        [0.9765]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9951],\n","        [0.9985],\n","        [0.9982],\n","        [0.9925]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 6933\n","------------------run No.6934--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.6739, 0.2078]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.6437], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.6157, 0.1481, 0.2703]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.9809], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(7.9447, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9709],\n","        [0.9758],\n","        [0.9728],\n","        [0.9315]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9354],\n","        [0.9527],\n","        [0.9463],\n","        [0.8572]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.5419, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8594],\n","        [0.0907],\n","        [0.1370],\n","        [0.8628]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9679],\n","        [0.9816],\n","        [0.9789],\n","        [0.9386]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.2726, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9271],\n","        [0.0421],\n","        [0.0768],\n","        [0.9289]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9819],\n","        [0.9913],\n","        [0.9900],\n","        [0.9683]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1795, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9516],\n","        [0.0261],\n","        [0.0532],\n","        [0.9525]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9868],\n","        [0.9943],\n","        [0.9933],\n","        [0.9778]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1334, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9639],\n","        [0.0185],\n","        [0.0408],\n","        [0.9645]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9893],\n","        [0.9956],\n","        [0.9949],\n","        [0.9824]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1059, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9713],\n","        [0.0142],\n","        [0.0331],\n","        [0.9716]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9908],\n","        [0.9964],\n","        [0.9958],\n","        [0.9851]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0878, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9762],\n","        [0.0114],\n","        [0.0279],\n","        [0.9764]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9918],\n","        [0.9969],\n","        [0.9964],\n","        [0.9869]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 6961\n","------------------run No.6962--------------------\n","\n","---------------Model parameters---------------:\n","\n","hidden layer weights:\n","Parameter containing:\n","tensor([[0.1385, 0.8381]], requires_grad=True)\n","\n","hidden layer baises:\n","Parameter containing:\n","tensor([0.2772], requires_grad=True)\n","\n","output layer weights:\n","Parameter containing:\n","tensor([[0.7717, 0.9518, 0.2138]], requires_grad=True)\n","\n","output layer baises:\n","Parameter containing:\n","tensor([0.4598], requires_grad=True)\n","\n","----------Epochs----------\n","\n","Epoch: 0, loss: tensor(9.0913, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9404],\n","        [0.9865],\n","        [0.9844],\n","        [0.9508]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.7298],\n","        [0.9116],\n","        [0.9093],\n","        [0.8895]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 1000, loss: tensor(0.6459, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.8336],\n","        [0.1090],\n","        [0.1565],\n","        [0.8373]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.8546],\n","        [0.9579],\n","        [0.9556],\n","        [0.9324]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 2000, loss: tensor(0.3009, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9205],\n","        [0.0471],\n","        [0.0833],\n","        [0.9207]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9491],\n","        [0.9874],\n","        [0.9861],\n","        [0.9703]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 3000, loss: tensor(0.1918, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9487],\n","        [0.0282],\n","        [0.0563],\n","        [0.9489]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9696],\n","        [0.9930],\n","        [0.9922],\n","        [0.9805]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 4000, loss: tensor(0.1401, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9623],\n","        [0.0196],\n","        [0.0426],\n","        [0.9624]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9778],\n","        [0.9952],\n","        [0.9945],\n","        [0.9849]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 5000, loss: tensor(0.1101, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9703],\n","        [0.0149],\n","        [0.0343],\n","        [0.9703]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9821],\n","        [0.9962],\n","        [0.9957],\n","        [0.9874]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 6000, loss: tensor(0.0907, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9755],\n","        [0.0118],\n","        [0.0287],\n","        [0.9755]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9848],\n","        [0.9969],\n","        [0.9964],\n","        [0.9891]], grad_fn=<MulBackward0>)\n","---------------------\n","\n","Epoch: 7000, loss: tensor(0.0770, grad_fn=<NegBackward0>)\n","model output: \n","tensor([[0.9792],\n","        [0.0098],\n","        [0.0247],\n","        [0.9792]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9866],\n","        [0.9973],\n","        [0.9969],\n","        [0.9902]], grad_fn=<MulBackward0>)\n","---------------------\n","Success....... stoped at epocch number 7148\n","*** Successful runs  = 10 ***\n","*** Failed runs  = 0 ***\n","*** Epochs for successful train runs:  Avg  = tensor(6624.3999), Stdv = tensor(839.2326) ***\n","*** Train loss for successful train runs:  Avg  = tensor(0.3426), Stdv = tensor(0.5282) ***\n","*** validation loss for successful train runs:  Avg  = tensor(10.9787), Stdv = tensor(2.6679) ***\n"]}]},{"cell_type":"markdown","source":["f) after a few runs it seems that the Neuron is acting mostly like an OR Gate\n","\n","g)\n"],"metadata":{"id":"O2VO3Ibg-hne"}},{"cell_type":"code","source":["models_hidden_layers = [2, 2, 4, 4, 2, 2, 4, 4, 1]\n","print(\"models_hidden_layers: \")\n","print(models_hidden_layers)\n","print(\"model_epochs_mean: \")\n","print(model_epochs_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HAIoZ-LW_9T6","executionInfo":{"status":"ok","timestamp":1669298116572,"user_tz":-120,"elapsed":304,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"bcb5b839-ffaa-4f26-bfcb-3e3a759d4781"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["models_hidden_layers: \n","[2, 2, 4, 4, 2, 2, 4, 4, 1]\n","model_epochs_mean: \n","[tensor(5118.2002), tensor(6015.2998), tensor(4686.), tensor(5757.1001), tensor(1176.7000), tensor(1496.), tensor(1164.2000), tensor(1408.8000), tensor(6624.3999)]\n"]}]},{"cell_type":"code","source":["# plotting the points \n","plt.scatter(models_hidden_layers ,model_epochs_mean, label= \"stars\", color= \"green\", marker= \"*\", s=30)\n","plt.xlabel('models_hidden_layers')\n","plt.ylabel('model_epochs_mean')\n","plt.title('models_hidden_layers vs model_epochs_mean')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"2Ws9wFdMOIc7","executionInfo":{"status":"ok","timestamp":1669298127246,"user_tz":-120,"elapsed":734,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"2c2af703-236a-4d99-bc8c-3a2ddf618d5c"},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["1. the means results suggests that the bigger the hidden layer size is the smaller avg of epochs per model"],"metadata":{"id":"Iu_fJ1DXFMDU"}},{"cell_type":"code","source":["models_bypass = [False, True, False, True, False, True, False, True, True]\n","print(\"models_bypass: \")\n","print(models_bypass)\n","print(\"model_epochs_mean: \")\n","print(model_epochs_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pqqyw28_KoTT","executionInfo":{"status":"ok","timestamp":1669298131526,"user_tz":-120,"elapsed":314,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"0514486b-6e81-4768-8ffc-e1f0d18a5574"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["models_bypass: \n","[False, True, False, True, False, True, False, True, True]\n","model_epochs_mean: \n","[tensor(5118.2002), tensor(6015.2998), tensor(4686.), tensor(5757.1001), tensor(1176.7000), tensor(1496.), tensor(1164.2000), tensor(1408.8000), tensor(6624.3999)]\n"]}]},{"cell_type":"code","source":["# plotting the points \n","plt.scatter(models_bypass ,model_epochs_mean, label= \"stars\", color= \"green\", marker= \"*\", s=30)\n","plt.xlabel('models_bypass')\n","plt.ylabel('model_epochs_mean')\n","plt.title('models_bypass vs model_epochs_mean')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"QLvN6Lp6PGHq","executionInfo":{"status":"ok","timestamp":1669298134903,"user_tz":-120,"elapsed":411,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"009601a4-7bf9-4a3e-c09a-e7e2a9c6a003"},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["2. models with with bypass on avg have a greater mean of epochs than the same models without the bypass"],"metadata":{"id":"VXEuTvt3KmlG"}},{"cell_type":"code","source":["models_l_rates = [0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1,  0.01,]\n","print(\"models_l_rates: \")\n","print(models_l_rates)\n","print(\"model_epochs_std: \")\n","print(model_epochs_std)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4MZ0PaXSGO5z","executionInfo":{"status":"ok","timestamp":1669298163176,"user_tz":-120,"elapsed":265,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"120dbb84-516e-47bc-d3c6-f54eef6bb910"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["models_l_rates: \n","[0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.01]\n","model_epochs_std: \n","[tensor(398.4450), tensor(1176.5719), tensor(387.9247), tensor(785.5455), tensor(101.8900), tensor(403.0980), tensor(79.7229), tensor(327.5603), tensor(839.2326)]\n"]}]},{"cell_type":"code","source":["# plotting the points \n","plt.scatter(models_l_rates ,model_epochs_std, label= \"stars\", color= \"green\", marker= \"*\", s=30)\n","plt.xlabel('models_l_rates')\n","plt.ylabel('model_epochs_std')\n","plt.title('models_l_rates vs model_epochs_std')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"rGWobIQlPlP6","executionInfo":{"status":"ok","timestamp":1669298169717,"user_tz":-120,"elapsed":311,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"da360e6c-e7fb-466b-f14e-d6345a0d9e97"},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["3. the model results suggest that the lower the Learing rate is the greater the avg standart diviation"],"metadata":{"id":"1U1p2w1_JSm7"}},{"cell_type":"code","source":["print(\"model output: \")\n","print(model(x_train))\n","print(\"---------------------\")\n","print(\"Neuron inputs: \")\n","print(x_train)\n","print(\"Neuron outputs: \")\n","print(model.y1)\n","print(\"---------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYwoqIFIMWlM","executionInfo":{"status":"ok","timestamp":1669298204027,"user_tz":-120,"elapsed":312,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"ff3cb50c-ffff-46ca-8c9f-d3adaba09965"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["model output: \n","tensor([[0.9796],\n","        [0.0095],\n","        [0.0243],\n","        [0.9796]], grad_fn=<MulBackward0>)\n","---------------------\n","Neuron inputs: \n","tensor([[1.0000, 0.1000],\n","        [1.0000, 0.9000],\n","        [0.9000, 0.9000],\n","        [0.1000, 0.9000]], requires_grad=True)\n","Neuron outputs: \n","tensor([[0.9869],\n","        [0.9974],\n","        [0.9970],\n","        [0.9903]], grad_fn=<MulBackward0>)\n","---------------------\n"]}]},{"cell_type":"markdown","source":["4. as we can see in this case, after training the nueron it self is resembling  as a Xor Gate "],"metadata":{"id":"Z5zCvL13Ngca"}}]}