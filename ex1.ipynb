{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xZRTnFqrB6nBUM6oW3jqNxANSvGQ_G7D","timestamp":1667668290157}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"y7VJuTUoG3PI"},"source":["# import relevant libraries\n","import torch\n","from torch import nn\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgCUwkp8vmCN"},"source":["# set the but function \n","class BTU(torch.nn.Module):\n","  def __init__(self, T=0.2):\n","      super(BTU, self).__init__()\n","      self.T = T    ## for slop control\n","\n","  def forward(self, input: torch.Tensor) -> torch.Tensor:\n","      return 1 / (1 + torch.exp(-input/self.T))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhdtNadR3axj"},"source":["# Linear layer for supporting tensors.\n","# for the linear calculation of each layer, will get the sizes of the mat for calculation\n","# the same math logic will apply for the output layer and the hidden layer\n","\n","class Linear2(torch.nn.Module):\n","  def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n","    factory_kwargs = {'device': device, 'dtype': dtype}\n","    super(Linear2, self).__init__()\n","    self.in_features = in_features\n","    self.out_features = out_features\n","    self.weight = nn.Parameter(torch.empty((in_features, out_features), **factory_kwargs))\n","    if bias:\n","        self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs))\n","    else:\n","        self.register_parameter('bias', None)\n","    self.reset_parameters()\n","\n","  def reset_parameters(self) -> None:\n","    self.weight = nn.Parameter(torch.rand([self.in_features, self.out_features]))\n","    if self.bias is not None:\n","      self.bias = nn.Parameter(torch.rand([self.out_features]))\n","    \n","  def set_weights(self, w, b):\n","    self.weight = nn.Parameter(torch.tensor(w))\n","    self.bias = nn.Parameter(torch.tensor(b))\n","      \n","  def forward(self, input: torch.Tensor) -> torch.Tensor:\n","    return torch.matmul(input, self.weight) + self.bias # * is elementwise\n","\n","  def extra_repr(self) -> str:\n","    return 'in_features={}, out_features={}, bias={}'.format(\n","        self.in_features, self.out_features, self.bias is not None\n","      )\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###### ex1 input runs ######\n","\n","# variables \n","n = 2\n","k = [1, 3, 4]  # hidden layer sizes\n","# set the data\n","input_dim = n  # input size\n","out_dim = 1   # output size\n","bypass = True   # bypass flag\n","Temp = 0.001  # sigmoid slop control\n"],"metadata":{"id":"gT6Iigc00hvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Network(nn.Module):\n","  def __init__(self, num_hidden, bypass=True):    #initialize network\n","    super().__init__()\n","    self.bypass = bypass\n","    self.hidden = Linear2(input_dim, num_hidden)      # set the hidden layer linear func for calculation\n","    if self.bypass or num_hidden == 1:\n","      self.bypass = True\n","      self.output = Linear2(num_hidden + input_dim, out_dim)    # in there is a bypass also consider the inputs on the output calculation, default bypass for k = 1\n","    else:\n","      self.output = Linear2(num_hidden, out_dim)      # set the output layer linear func for calculation\n","    self.BTU = BTU(Temp)                              # give the network the BTU function\n","  \n","  def set_weights(self, w, b, layer): # set weights and biases\n","    try:\n","      if layer == 'output':\n","        self.output.set_weights(w, b)\n","      if layer == 'hidden':\n","        self.hidden.set_weights(w, b)\n","    except:\n","      print(\"Exception thrown: sizes not matched\")    # if given a not valid mat proportions\n","  def forward(self, input):\n","    z1 = self.hidden(input)     # activate the func on the input\n","    y1 = self.BTU(z1)           # then apply btu to the res\n","    if self.bypass:              # to bypass add the inputs to the output cal\n","      y1_concat = torch.cat((input, y1), num_hidden)\n","      print(\"y1_concat\")\n","      print(y1_concat)\n","      z2 = self.output(y1_concat)   # cal the output\n","    else:\n","      z2 = self.output(y1)  # cal the output \n","    return self.BTU(z2)\n"],"metadata":{"id":"jMgEn0xXZDQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" ## diff assessment from the  model results vs expeceted results\n","def Loss(x, t, print_deltas=False):\n","  squared_deltas = torch.square(my_model(x) - t) # first\n","  if print_deltas:\n","    print(\"the squared_deltas:\")\n","    print(squared_deltas)\n","  return torch.sum(squared_deltas)    # return the sum of SSE"],"metadata":{"id":"g3r6fvASgq6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIeD-mtMCgsh","executionInfo":{"status":"ok","timestamp":1668522021879,"user_tz":-120,"elapsed":298,"user":{"displayName":"Renny Wang","userId":"04046858756059938750"}},"outputId":"1e497a36-ec28-4e5b-fc74-c712d1948e47"},"source":["#2^n input possibilitys\n","xor_train = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n","for i in k: # create all modles (each with different k's)\n","  # craete modle\n","  my_model = Network(i, bypass = False)\n","  num_hidden = i\n","  # set weights for the hidden and the output layers\n","  if num_hidden == 3:\n","    w = [[-1., 1., 1.], [-1., 1., 1.]]\n","    b = [0.5, -0.5, -1.5]\n","    print(\"hidden layer:\")\n","    print(\"the weights: \", end = \"\")\n","    print(w)\n","    print(\"the baises: \", end = \"\")\n","    print(b)\n","    my_model.set_weights(w, b, 'hidden')\n","    w = [[-2.], [1.],[-2.]]\n","    b = [-0.5]\n","    print(\"output layer:\")\n","    print(\"the weights: \", end = \"\")\n","    print(w)\n","    print(\"the baises: \", end = \"\")\n","    print(b)\n","    my_model.set_weights(w, b, 'output')\n","  elif num_hidden == 1:\n","    w = [[1.], [1.]]\n","    b = [-1.5]\n","    print(\"hidden layer:\")\n","    print(\"the weights: \", end = \"\")\n","    print(w)\n","    print(\"the baises: \", end = \"\")\n","    print(b)\n","    my_model.set_weights(w, b, 'hidden')\n","    print(\"output layer:\")\n","    w = [[1.],[1.],[-2.]]\n","    b = [-0.5]\n","    print(\"the weights: \", end = \"\")\n","    print(w)\n","    print(\"the baises: \", end = \"\")\n","    print(b)\n","    my_model.set_weights(w, b, 'output')\n","  elif num_hidden == 4:\n","    w = [[-1., -1., 1., 1.], [-1., 1., -1., 1.]]\n","    b = [1.5, -0.5, -0.5, -1.5]\n","    print(\"hidden layer:\")\n","    print(\"the weights: \", end = \"\")\n","    print(w)\n","    print(\"the baises: \", end = \"\")\n","    print(b)\n","    my_model.set_weights(w, b, 'hidden')\n","    print(\"output layer:\")\n","    w = [[0.], [1.], [1.], [0.]]\n","    b = [-0.5]\n","    print(\"the weights: \", end = \"\")\n","    print(w)\n","    print(\"the baises: \", end = \"\")\n","    print(b)\n","    my_model.set_weights(w, b, 'output')\n","  else:\n","    print(\"the k input is not valid...\")\n","    break\n","\n","  res = my_model(xor_train)\n","  print(\"modle with k = \" + str(i) + \" for input = \" , end =\" \")\n","  print(xor_train , end =\" \")\n","  print(\": \")\n","  print(\"output = \" , end =\" \")\n","  print(res)\n","\n","  t = torch.tensor([[0.], [1.], [1.], [0.]], dtype=torch.float32) \n","  print(\"the expected results for xor:\")\n","  print(t)  \n","  lost_val = Loss(xor_train, t, True)   # t is the results for a modle for all xor_train inputs\n","  print(\"the Loss:\")\n","  print(lost_val)\n","  print(\"-----------------------------------------------------\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden layer:\n","the weights: [[1.0], [1.0]]\n","the baises: [-1.5]\n","output layer:\n","the weights: [[1.0], [1.0], [-2.0]]\n","the baises: [-0.5]\n","y1_concat\n","tensor([[0., 0., 0.],\n","        [0., 1., 0.],\n","        [1., 0., 0.],\n","        [1., 1., 1.]], grad_fn=<CatBackward0>)\n","modle with k = 1 for input =  tensor([[0., 0.],\n","        [0., 1.],\n","        [1., 0.],\n","        [1., 1.]]) : \n","output =  tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.]], grad_fn=<MulBackward0>)\n","the expected results for xor:\n","tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","y1_concat\n","tensor([[0., 0., 0.],\n","        [0., 1., 0.],\n","        [1., 0., 0.],\n","        [1., 1., 1.]], grad_fn=<CatBackward0>)\n","the squared_deltas:\n","tensor([[0.],\n","        [0.],\n","        [0.],\n","        [0.]], grad_fn=<PowBackward0>)\n","the Loss:\n","tensor(0., grad_fn=<SumBackward0>)\n","-----------------------------------------------------\n","hidden layer:\n","the weights: [[-1.0, 1.0, 1.0], [-1.0, 1.0, 1.0]]\n","the baises: [0.5, -0.5, -1.5]\n","output layer:\n","the weights: [[-2.0], [1.0], [-2.0]]\n","the baises: [-0.5]\n","modle with k = 3 for input =  tensor([[0., 0.],\n","        [0., 1.],\n","        [1., 0.],\n","        [1., 1.]]) : \n","output =  tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.]], grad_fn=<MulBackward0>)\n","the expected results for xor:\n","tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","the squared_deltas:\n","tensor([[0.],\n","        [0.],\n","        [0.],\n","        [0.]], grad_fn=<PowBackward0>)\n","the Loss:\n","tensor(0., grad_fn=<SumBackward0>)\n","-----------------------------------------------------\n","hidden layer:\n","the weights: [[-1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, 1.0]]\n","the baises: [1.5, -0.5, -0.5, -1.5]\n","output layer:\n","the weights: [[0.0], [1.0], [1.0], [0.0]]\n","the baises: [-0.5]\n","modle with k = 4 for input =  tensor([[0., 0.],\n","        [0., 1.],\n","        [1., 0.],\n","        [1., 1.]]) : \n","output =  tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.]], grad_fn=<MulBackward0>)\n","the expected results for xor:\n","tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","the squared_deltas:\n","tensor([[0.],\n","        [0.],\n","        [0.],\n","        [0.]], grad_fn=<PowBackward0>)\n","the Loss:\n","tensor(0., grad_fn=<SumBackward0>)\n","-----------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["Summry:\n","\n","the main idea is the create each layer with the suitble mat proportion so we'll be able to calculate each layer, for this procces we must insert the right data with the right sizes of mats.\n","\n","In each layer calculation we will get the new mat of values then run  ithem in the btu function so each layer \"inputs\" and binary.\n","\n","For cases with bypass the calculation of the output will also include the original inputs of the network as well, and of course this will change the size of the mat in the output calculation.\n","\n","To prove the networks work the code runs all cases for each network and we compare the each ouput to the expected results in the xor truth table,\n","furthermore we can see the loss function that does the actual comparison by SSE calculation return a value of zero. "],"metadata":{"id":"sYIdoI0TIv9l"}}]}